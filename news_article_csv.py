# -*- coding: utf-8 -*-
"""news_article.csv

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sTcTb2otNiVOcQpfFEclmYMs60p-xIYa
"""

#Assignment-1
import requests
from bs4 import BeautifulSoup
import csv
import time

bbc_url = "https://www.bbc.com/news/live/c5yw80xv4wjt"
response = requests.get(bbc_url)
soup = BeautifulSoup(response.content, 'html.parser')


headlines = soup.find_all('h3')
dates = soup.find_all('time')
anchors = soup.find_all('a', href=True)


headline_list = [headline.text for headline in headlines]
date_list = [date['datetime'] if date.has_attr('datetime') else "Unknown Date" for date in dates]

url_list = [a['href'] if a['href'].startswith('https') else 'https://www.bbc.com' + a['href'] for a in anchors]


source = "BBC News"


for i, headline in enumerate(headlines):
    publication_date = date_list[i] if i < len(date_list) else "Unknown Date"
    url = url_list[i] if i < len(url_list) else "URL not available"
    print(f"{headline.text} (Published: {publication_date}, Source: {source}, URL: {url})\n")
    time.sleep(1)


def generate_summary(headlines_list, dates_list, url_list, source):

    if len(headlines_list) == 0:
        return "No headlines to summarize."

    summary = f"Total headlines: {len(headlines_list)}\n"
    summary += "Key headlines with publication dates, source, and URLs:\n"


    for i, headline in enumerate(headlines_list[:3]):
        publication_date = dates_list[i] if i < len(dates_list) else "Unknown Date"
        url = url_list[i] if i < len(url_list) else "URL not available"
        summary += f"{i + 1}. {headline} (Published: {publication_date}, Source: {source}, URL: {url})\n"

    return summary


# Print the summary with dates, source, and URLs
summary = generate_summary(headline_list, date_list, url_list, source)
print("\nSummary:\n" + summary)

#assigmnet-1
import requests
from bs4 import BeautifulSoup
import csv
import time

cnn_url = "https://edition.cnn.com/world/live-news/israel-gaza-lebanon-war-10-10-24/index.html"
response = requests.get(cnn_url)
soup = BeautifulSoup(response.content, 'html.parser')


headlines = soup.find_all('h2')
dates = soup.find_all('time')
anchors = soup.find_all('a', href=True)

headline_list = [headline.text.strip() for headline in headlines]
date_list = [date['datetime'] if date.has_attr('datetime') else "Unknown Date" for date in dates]

url_list = [a['href'] if a['href'].startswith('https') else 'https://edition.cnn.com' + a['href'] for a in anchors]


source = "CNN News"


for i, headline in enumerate(headlines):
    publication_date = date_list[i] if i < len(date_list) else "Unknown Date"
    url = url_list[i] if i < len(url_list) else "URL not available"
    print(f"{headline.text.strip()} (Published: {publication_date}, Source: {source}, URL: {url})\n")
    time.sleep(1)


def generate_summary(headlines_list, dates_list, url_list, source):

    if len(headlines_list) == 0:
        return "No headlines to summarize."

    summary = f"Total headlines: {len(headlines_list)}\n"
    summary += "Key headlines with publication dates, source, and URLs:\n"


    for i, headline in enumerate(headlines_list[:3]):
        publication_date = dates_list[i] if i < len(dates_list) else "Unknown Date"
        url = url_list[i] if i < len(url_list) else "URL not available"
        summary += f"{i + 1}. {headline} (Published: {publication_date}, Source: {source}, URL: {url})\n"

    return summary


# Print the summary with dates, source, and URLs
summary = generate_summary(headline_list, date_list, url_list, source)
print("\nSummary:\n" + summary)

!pip install nltk spacy scikit-learn
!python -m spacy download en_core_web_sm

#assignment-2
import spacy
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn import metrics


nlp = spacy.load("en_core_web_sm")
nltk.download('stopwords')
from nltk.corpus import stopwords


articles = [
    "The president signed a new bill into law today.",  # Politics
    "The latest technology trends include AI, ML, and cloud computing.",  # Technology
    "The soccer match ended with a score of 3-1.",  # Sports

]
labels = ['Politics', 'Technology', 'Sports']

def preprocess(text):
    doc = nlp(text)
    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]
    return ' '.join(tokens)


preprocessed_articles = [preprocess(article) for article in articles]


model = make_pipeline(TfidfVectorizer(), MultinomialNB())


X_train, X_test, y_train, y_test = train_test_split(preprocessed_articles, labels, test_size=0.3, random_state=42)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)


print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
print("Classification Report:\n", metrics.classification_report(y_test, y_pred))

new_article = "The team won the championship after a thrilling final match."
preprocessed_article = preprocess(new_article)
print("Predicted Topic:", model.predict([preprocessed_article])[0])



pip install fastapi uvicorn sqlalchemy pydantic